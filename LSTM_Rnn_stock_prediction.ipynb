{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/LucasMagalhaes/Documents/Projects/ML/Stock_predixtion/Tests/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"C:/Users/LucasMagalhaes/Documents/Projects/ML/Stock_predixtion/Tests/\"\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_boulinger_bands(df):\n",
    "    middle_band = []\n",
    "    upper_band = []\n",
    "    lower_band = []\n",
    "\n",
    "    for i in range(20, df.shape[0]+1):\n",
    "\n",
    "        lst_num = df['Adj_Close'][(i-20):i].tolist()\n",
    "        _mid_band = sum(lst_num)/20\n",
    "        std = math.sqrt(sum([math.pow((x-_mid_band),2) for x in lst_num])/20)\n",
    "        _upper_band = _mid_band + (2*std)\n",
    "        _lower_band = _mid_band - (2*std)\n",
    "\n",
    "        if i == 20:\n",
    "            middle_band = [_mid_band for x in range(20)]\n",
    "            upper_band = [_upper_band for x in range(20)]\n",
    "            lower_band = [_lower_band for x in range(20)]\n",
    "        else:\n",
    "            middle_band.append(_mid_band)\n",
    "            upper_band.append(_upper_band)\n",
    "            lower_band.append(_lower_band)\n",
    "\n",
    "    df['Middle_Band'] = middle_band\n",
    "    df['Upper_Band'] = upper_band\n",
    "    df['Lower_Band'] = lower_band\n",
    "    df['Lower_Upper_Diff'] = df['Upper_Band'] - df['Lower_Band']\n",
    "    df['Upper_Price_Diff'] = df['Upper_Band'] - df['Adj_Close']\n",
    "    df['Price_Lower_Diff'] = df['Adj_Close'] - df['Lower_Band']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_RSI(df):\n",
    "    rsi = []\n",
    "\n",
    "    av_gain = 0\n",
    "    av_loss = 0\n",
    "\n",
    "    for i in range(15, df.shape[0]+1):\n",
    "        lst_num = df['Adj_Close'][(i-15):i].tolist()\n",
    "        lst_change = [lst_num[x+1] - lst_num[x] for x in range(len(lst_num)-1)]\n",
    "\n",
    "        if i == 15:\n",
    "            _av_gains = sum([x for x in lst_change if x > 0])/14\n",
    "            _av_loss = -1*sum(list(filter(lambda x: x < 0, lst_change)))/14\n",
    "\n",
    "            if _av_loss == 0:\n",
    "                _rsi = 100\n",
    "            else:\n",
    "                rs = _av_gains/_av_loss\n",
    "                _rsi = 100 - (100/(1+rs))\n",
    "\n",
    "            av_gain = _av_gains\n",
    "            av_loss = _av_loss\n",
    "\n",
    "            rsi = [_rsi for x in range(15)]\n",
    "\n",
    "        else:\n",
    "            curr_gain = lambda x: x if x > 0 else 0\n",
    "            curr_loss = lambda x: abs(x) if x < 0 else 0\n",
    "\n",
    "            _av_gains = ((av_gain*13)+curr_gain(lst_change[(len(lst_change)-1)]))/14\n",
    "            _av_loss = ((av_loss*13)+curr_loss(lst_change[(len(lst_change)-1)]))/14\n",
    "            \n",
    "            if _av_loss == 0:\n",
    "                _rsi = 100\n",
    "            else:\n",
    "                rs = _av_gains/_av_loss\n",
    "                _rsi = 100 - (100/(1+rs))\n",
    "\n",
    "            av_gain = _av_gains\n",
    "            av_loss = _av_loss\n",
    "            rsi.append(_rsi)\n",
    "\n",
    "    df['RSI'] = rsi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_averages_MACD(df):\n",
    "    #26 ema line\n",
    "    two_six_ema = [0 for i in range(25)]\n",
    "    #12 ema line\n",
    "    one_two_ema = [0 for i in range(11)]\n",
    "    #signal line is a 9 day ema line\n",
    "    signal_ema = [0 for i in range(33)]\n",
    "    #macd line\n",
    "    macd_line = [0 for i in range(25)]\n",
    "    #simple 50 day moving average\n",
    "    fifty_ma = [0 for i in range(49)]\n",
    "    #simple 200 day moving average\n",
    "    two_hondo_ma = [0 for i in range(199)]\n",
    "\n",
    "    #Turning the price column into list for ease of use\n",
    "    lst_num = df['Adj_Close'].tolist()\n",
    "\n",
    "    _12_day_ema = 0\n",
    "    _26_day_ema = 0\n",
    "    _9_day_ema = 0\n",
    "    for i in range(len(lst_num)):\n",
    "        \"\"\"\n",
    "        12 DAY EXPONENTIAL MOVING AVERAGE\n",
    "        \"\"\"\n",
    "        if i >= 11:\n",
    "            if i == 11:\n",
    "                _12_day_ema = sum(lst_num[0:i+1])/12\n",
    "            else:\n",
    "                c = 2/13\n",
    "                _12_day_ema = (lst_num[i]-_12_day_ema)*c + _12_day_ema\n",
    "\n",
    "            one_two_ema.append(_12_day_ema)\n",
    "\n",
    "        \"\"\"\n",
    "        26 DAY EXPONENTIAL MOVING AVERAGE\n",
    "        \"\"\"\n",
    "        if i >= 25:\n",
    "            if i == 25:\n",
    "                _26_day_ema = sum(lst_num[0:i+1])/26\n",
    "            else:\n",
    "                c = 2/27\n",
    "                _26_day_ema = (lst_num[i]-_26_day_ema)*c + _26_day_ema\n",
    "\n",
    "            two_six_ema.append(_26_day_ema)\n",
    "\n",
    "        \"\"\"\n",
    "        MACD LINE\n",
    "        \"\"\"\n",
    "        if i >= 25:\n",
    "            macd_line.append(one_two_ema[i] - two_six_ema[i])\n",
    "\n",
    "        \"\"\"\n",
    "        9 DAY EXPONENTIAL MOVING AVERAGE - SIGNAL LINE\n",
    "        \"\"\"\n",
    "        if i >= 33:\n",
    "            if i == 33:\n",
    "                _9_day_ema = sum(macd_line[i-9:i+1])/9\n",
    "            else:\n",
    "                c = 2/10\n",
    "                _9_day_ema = (macd_line[i]-_9_day_ema)*c + _9_day_ema\n",
    "\n",
    "            signal_ema.append(_9_day_ema)\n",
    "\n",
    "        \"\"\"\n",
    "        50 DAY SIMPLE MOVING AVERAGE\n",
    "        \"\"\"\n",
    "        if i >= 49:\n",
    "            _50_day_sma = sum(lst_num[i-49:i+1])/50\n",
    "\n",
    "            fifty_ma.append(_50_day_sma)           \n",
    "\n",
    "        \"\"\"\n",
    "        200 DAY SIMPLE MOVING AVERAGE\n",
    "        \"\"\"\n",
    "        if i >= 199:\n",
    "            _200_day_sma = sum(lst_num[i-199:i+1])/200\n",
    "        \n",
    "            two_hondo_ma.append(_200_day_sma)\n",
    "\n",
    "    df['MACD_Line'] = macd_line \n",
    "    df['Signal_Line'] = signal_ema\n",
    "    df['MACD_Diff'] = df['MACD_Line'] - df['Signal_Line']\n",
    "    df['200_day_SMA'] = two_hondo_ma\n",
    "    df['50_day_SMA'] = fifty_ma\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sets(df, scaler_type='min_max_scaler'):\n",
    "    #Copy dataframe to a new dataframe that will be manipulated\n",
    "    df_stock = df.copy()\n",
    "    #drop useless columns, keep all indicators plus Adj CLose and Adj Volume\n",
    "    df_stock.drop(['Date', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                    'Dividend', 'Split', 'Adj_Open', 'Adj_High', 'Adj_Low'], 1, inplace=True)\n",
    "    #Need to get rid of the first 200 rows so all indicators will have their true values intead of zeros\n",
    "    df_stock = df_stock.iloc[199:, :]\n",
    "    \n",
    "    #print(df_stock.head())\n",
    "    #print(df_stock.tail())\n",
    "    \n",
    "    #Test set will be 20% of data\n",
    "    test_set_size = math.floor(df_stock.shape[0]*.2)\n",
    "    #Train set size will be the total set minus test set\n",
    "    train_set_size = df_stock.shape[0] - test_set_size\n",
    "    #Need to scale the data\n",
    "    all_data = scaler(df_stock, scaler_type=scaler_type)\n",
    "    \n",
    "    if all_data is not None:\n",
    "        #Seperate Data between training and testing\n",
    "        train_data = all_data[:train_set_size, :]\n",
    "        test_data = all_data[train_set_size:, :]\n",
    "        \n",
    "        #Return train and test data\n",
    "        return train_data, test_data\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(df, scaler_type, smoothing_size='365'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Min Max Scaler will scale all features to fit between -1 and 1,\n",
    "    it uses a sliding window, that will prevent early stock prices\n",
    "    to be useless since they would get even smaller.\n",
    "    The default value of the sliding window is 365 (a year)\n",
    "    \"\"\"\n",
    "    if scaler_type == 'min_max_scaler':\n",
    "        #Turn dataFrame into numpy array\n",
    "        all_data = df.values\n",
    "        scaler = MinMaxScaler()\n",
    "        smoothing_window_size = 365\n",
    "        for di in range(0,((all_data.shape[0]//smoothing_window_size)*smoothing_window_size), smoothing_window_size):\n",
    "            scaler.fit(all_data[di:di+smoothing_window_size])\n",
    "            all_data[di:di+smoothing_window_size, :] = scaler.transform(all_data[di:di+smoothing_window_size, :])\n",
    "\n",
    "        scaler.fit(all_data[di+smoothing_window_size:])\n",
    "        all_data[di+smoothing_window_size:, :] = scaler.transform(all_data[di+smoothing_window_size:, :])\n",
    "        \n",
    "        return all_data\n",
    "    \"\"\"\n",
    "    Percent change scaler, will scale the:\n",
    "    #Name, Index\n",
    "    - Adj_Close\n",
    "    - 200_day_SMA\n",
    "    - 50_day_SMA\n",
    "    - Middle_Band\n",
    "    - Lower_Band\n",
    "    - Upper_Band\n",
    "    \n",
    "    The rest of the features are small and have low variance\n",
    "    It will change the values based on percent change from\n",
    "    previous value\n",
    "    \"\"\"    \n",
    "    if scaler_type == 'percent_change':\n",
    "        #Turn dataFrame into numpy array\n",
    "        all_data = df.values\n",
    "        #list of columns to be scaled\n",
    "        cols = ['Adj_Close','200_day_SMA','50_day_SMA','Middle_Band',\n",
    "               'Lower_Band','Upper_Band']\n",
    "        #Get index of the columns to be scaled\n",
    "        cols_i = [df.columns.get_loc(i) for i in cols]\n",
    "        \n",
    "        #Loop throgh all rows and scale them\n",
    "        for di in range(1, all_data.shape[0]):\n",
    "            for col in cols_i:\n",
    "                all_data[di,col] = (all_data[di,col]-df.iloc[di-1,col])/df.iloc[di-1,col]\n",
    "            \n",
    "        return all_data\n",
    "    \n",
    "    \"\"\"\n",
    "    High Low Day scaler will change the feature values for negative percent\n",
    "    change to -1 and feature values with positive percent change from previous\n",
    "    value to 1. This will only work on the features:\n",
    "    - Adj_Close\n",
    "    - 200_day_SMA\n",
    "    - 50_day_SMA\n",
    "    \n",
    "    I am also going to drop:\n",
    "    - Middle_Band\n",
    "    - Lower_Band\n",
    "    - Upper_Band\n",
    "    \"\"\"    \n",
    "    if scaler_type == 'high_low_day':\n",
    "        #Throw away columns not used\n",
    "        cols_throw = ['Middle_Band','Lower_Band','Upper_Band']\n",
    "        df = df.drop(cols_throw, 1)\n",
    "        \n",
    "        #Columns to be scaled\n",
    "        cols = ['Adj_Close','200_day_SMA','50_day_SMA']\n",
    "        cols_i = cols_i = [df.columns.get_loc(i) for i in cols]\n",
    "        \n",
    "        #Turn dataFrame into numpy array\n",
    "        all_data = df.values\n",
    "        #print(all_data.shape)\n",
    "        \n",
    "        chg = 0\n",
    "        for di in range(1, all_data.shape[0]):\n",
    "            for col in cols_i:\n",
    "                chg = (all_data[di,col]-df.iloc[di-1,col])/df.iloc[di-1,col]\n",
    "                if chg >= 0:\n",
    "                    all_data[di,col] = 1\n",
    "                else:\n",
    "                    all_data[di,col] = -1\n",
    "    \n",
    "        return all_data\n",
    "        \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.18378471e-01  2.36712000e+07  2.72731140e+01  1.23709139e-01\n",
      "   1.07255064e-01  1.64540748e-02 -2.52594134e-02 -2.14470314e-02\n",
      "  -3.81238194e-03  3.81183014e-01  3.03209775e-01]\n",
      " [ 1.00000000e+00  1.24992000e+07  2.84310732e+01  1.22106977e-01\n",
      "   1.00298106e-01  2.18088710e-02 -2.49954607e-02 -2.21567173e-02\n",
      "  -2.83874339e-03 -1.00000000e+00 -1.00000000e+00]\n",
      " [-1.00000000e+00  1.52824000e+07  2.84310732e+01  1.15791145e-01\n",
      "   9.24493108e-02  2.33418342e-02 -2.45038116e-02 -2.26261361e-02\n",
      "  -1.87767543e-03 -1.00000000e+00 -1.00000000e+00]\n",
      " [-1.00000000e+00  1.12616000e+07  3.99766719e+01  1.09582012e-01\n",
      "   6.83296051e-02  4.12524067e-02 -2.24001353e-02 -2.25809360e-02\n",
      "   1.80800658e-04 -1.00000000e+00 -1.00000000e+00]\n",
      " [-1.00000000e+00  1.07744000e+07  4.38768016e+01  1.02481638e-01\n",
      "   5.51234237e-02  4.73582144e-02 -1.99209896e-02 -2.20489467e-02\n",
      "   2.12795712e-03 -1.00000000e+00 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"APPLE\"\"\"\n",
    "#Acquire data from CSV\n",
    "aapl = pd.read_csv(filepath+'EOD-AAPL.csv')\n",
    "#Values need to be reindexed from bottom to top\n",
    "#so earlier values are on top of the dataframe\n",
    "aapl = aapl.iloc[::-1]\n",
    "#Reset the index, and get rid of old index\n",
    "aapl = aapl.reset_index(drop=True)\n",
    "\n",
    "#Add indicators to the dataframe\n",
    "aapl = add_RSI(aapl)\n",
    "aapl = add_boulinger_bands(aapl)\n",
    "aapl = moving_averages_MACD(aapl)\n",
    "\n",
    "#Get the train and test *preprocessed* data\n",
    "aapl_train, aapl_test = create_data_sets(aapl, scaler_type='high_low_day')\n",
    "\n",
    "print(aapl_train[1:6])\n",
    "#print(aapl_test[aapl_test.shape[0]-5:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MICROSOFT\"\"\"\n",
    "#Acquire data from CSV\n",
    "msft = pd.read_csv(filepath+'EOD-MSFT.csv')\n",
    "#Values need to be reindexed from bottom to top\n",
    "#so earlier values are on top of the dataframe\n",
    "msft = msft.iloc[::-1]\n",
    "#Reset the index, and get rid of old index\n",
    "msft = msft.reset_index(drop=True)\n",
    "\n",
    "#Add indicators to the dataframe\n",
    "msft = add_RSI(msft)\n",
    "msft = add_boulinger_bands(msft)\n",
    "msft = moving_averages_MACD(msft)\n",
    "\n",
    "#Get the train and test *preprocessed* data\n",
    "msft_train, msft_test = create_data_sets(msft, scaler_type='high_low_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 365\n",
    "\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title(\"Apple\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['Adj_Close'][(aapl.shape[0]-time_step):], label=\"Apple\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['Middle_Band'][(aapl.shape[0]-time_step):], label=\"Middle Band\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['Upper_Band'][(aapl.shape[0]-time_step):], label=\"Upper Band\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['Lower_Band'][(aapl.shape[0]-time_step):], label=\"Lower Band\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['200_day_SMA'][(aapl.shape[0]-time_step):], label=\"200 day SMA\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['50_day_SMA'][(aapl.shape[0]-time_step):], label=\"50 day SMA\")\n",
    "plt.ylabel(\"US Dollar\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.title(\"Microsoft\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['Adj_Close'][(msft.shape[0]-time_step):], label=\"Microsoft\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['Middle_Band'][(msft.shape[0]-time_step):], label=\"Middle Band\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['Upper_Band'][(msft.shape[0]-time_step):], label=\"Upper Band\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['Lower_Band'][(msft.shape[0]-time_step):], label=\"Lower Band\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['200_day_SMA'][(msft.shape[0]-time_step):], label=\"200 day SMA\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['50_day_SMA'][(msft.shape[0]-time_step):], label=\"50 day SMA\")\n",
    "plt.ylabel(\"US Dollar\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['RSI'][(aapl.shape[0]-time_step):], label=\"Apple\")\n",
    "plt.ylabel(\"RSI\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.axhline(70, ls='--')\n",
    "plt.axhline(30, ls='--')\n",
    "plt.axhline(50, ls='-')\n",
    "plt.ylim([0,100])\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['RSI'][(msft.shape[0]-time_step):], label=\"Microsoft\")\n",
    "plt.ylabel(\"RSI\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.axhline(70, ls='--')\n",
    "plt.axhline(30, ls='--')\n",
    "plt.axhline(50, ls='-')\n",
    "plt.ylim([0,100])\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['MACD_Line'][(aapl.shape[0]-time_step):], label=\"MACD Line\")\n",
    "plt.plot(aapl.index[(aapl.shape[0]-time_step):], aapl['Signal_Line'][(aapl.shape[0]-time_step):], label=\"Signal Line\")\n",
    "plt.bar(aapl.index[(aapl.shape[0]-time_step):], aapl['MACD_Diff'][(aapl.shape[0]-time_step):], color = ['r' if x < 0 else 'g' for x in aapl['MACD_Diff'][(aapl.shape[0]-time_step):].tolist()])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['MACD_Line'][(msft.shape[0]-time_step):], label=\"MACD Line\")\n",
    "plt.plot(msft.index[(msft.shape[0]-time_step):], msft['Signal_Line'][(msft.shape[0]-time_step):], label=\"Signal Line\")\n",
    "plt.bar(msft.index[(msft.shape[0]-time_step):], msft['MACD_Diff'][(msft.shape[0]-time_step):], color = ['r' if x < 0 else 'g' for x in msft['MACD_Diff'][(msft.shape[0]-time_step):].tolist()])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStockPredictor():\n",
    "\n",
    "    def __init__(self, data_dim, lstm_size=[128], batch_size=10, num_unrollings=10, learning_rate=0.01, \n",
    "                learning_rate_decay=0.99, init_epoch_decay=5):\n",
    "        \n",
    "        #numer of hidden units in the LSTM cell\n",
    "        self.lstm_size = lstm_size\n",
    "        #number of layers in the LSTM network\n",
    "        self.num_layers = len(lstm_size)\n",
    "        #batch size\n",
    "        self.batch_size = batch_size\n",
    "        #number of sequential data that will be used to train model\n",
    "        self.num_unrollings = num_unrollings\n",
    "        #dimension of data\n",
    "        self.data_dim = data_dim\n",
    "        #learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        #Learning rate decay\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        #First epoch to start the decay of the learning rate\n",
    "        self.init_epoch_decay = init_epoch_decay\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    def build(self):\n",
    "        #Define placeholders\n",
    "        #Placeholder for dropout rate and learning rate\n",
    "        tf_lr = tf.placeholder(tf.float32, shape=None, name='tf_learning_rate')\n",
    "        tf_keep_prob = tf.placeholder(tf.float32, shape=None, name='tf_keep_prob')\n",
    "        print(\"Learning Rate >>> \",tf_lr)\n",
    "        print(\"Keep Prob >>> \",tf_keep_prob)\n",
    "\n",
    "        #Place holder for input data and target data\n",
    "        #input of size [batch_size, data_dim, num_unrollings]\n",
    "        tf_x = tf.placeholder(tf.float32, shape=(self.batch_size, self.data_dim, self.num_unrollings), name='tf_x')\n",
    "        #target of size [batch_size, 1, num_unrolling]\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size, 1), name='tf_y')\n",
    "        print(\"input tensor >>> \", tf_x)\n",
    "        print(\"Target tensor >>> \", tf_y)\n",
    "\n",
    "        #Create LSTM layers, stacked using MultiRNNCell, and DropoutWrapper for overfitting\n",
    "        lstm_cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.contrib.rnn.LSTMCell(num_units=n, initializer=tf.contrib.layers.xavier_initializer()), \n",
    "                    output_keep_prob=tf_keep_prob\n",
    "                    ) for n in self.lstm_size])\n",
    "        print(\"LSTM CELLS >>> \", lstm_cells)\n",
    "\n",
    "        #Create inital state\n",
    "        self.initial_state = lstm_cells.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        print(\"Initial State >>> \", self.initial_state)\n",
    "\n",
    "        #format input data to match the required format from dynamic_rnn\n",
    "        #[num_unrollings, batch_size, data_dim]\n",
    "        formatted_input = tf.transpose(tf_x, [0, 2, 1])\n",
    "        print(\"Formatted input >>> \", formatted_input)\n",
    "\n",
    "        #Connect all Nodes to create the RNN\n",
    "        lstm_output, self.final_state = tf.nn.dynamic_rnn(lstm_cells, formatted_input,\n",
    "                                                        initial_state=self.initial_state)\n",
    "        print(\"LSTM NETWORK OUTPUT >>> \", lstm_output)\n",
    "        #format output data to fit to prediction layer\n",
    "        #[num_unrolling, batch_size, last LSTMCell output size] -> [(num_unrolling * batch_size), last LSTMCell output size]\n",
    "        #formatted_lstm_output = tf.reshape(lstm_output, [self.num_unrollings*self.batch_size, self.lstm_size[-1]])\n",
    "        #or pick only last output batch\n",
    "        #formatted_lstm_output = tf.gather(lstm_output, int(lstm_output.get_shape()[0]) - 1, name='last_lstm_output')\n",
    "        formatted_lstm_output = lstm_output[:, -1]\n",
    "        print(\"Formatted LSTM output >>> \", formatted_lstm_output)\n",
    "\n",
    "        #define variables for prediction layer\n",
    "        tf_w = tf.Variable(tf.truncated_normal([self.lstm_size[-1], 1]), name=\"w\")\n",
    "        tf_b = tf.Variable(tf.constant(0.1, shape=[1], name=\"b\"))\n",
    "\n",
    "        #Calculate the RNN prediction\n",
    "        predictions = tf.nn.xw_plus_b(formatted_lstm_output, tf_w, tf_b, name='predictions')\n",
    "\n",
    "        #Define Cost function\n",
    "        cost = tf.reduce_mean(tf.square(predictions - tf_y), name='train_cost_mse')\n",
    "\n",
    "        #Define optimizer to use\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=tf_lr)\n",
    "\n",
    "        #train funtion\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, train, num_epoch=30):\n",
    "        #Start TensorFlow session\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            #Initialize variables\n",
    "            sess.run(self.init_op)\n",
    "            #Create learning rate array, which will start at learning_rate and decay exponentially by learning_rate_decay\n",
    "            lr_array = [self.learning_rate for _ in range(self.init_epoch_decay)] + [(self.learning_rate * (self.learning_rate_decay ** (i+1))) for i in range(num_epoch-self.init_epoch_decay)]\n",
    "            #Loop through num_epoch\n",
    "            iteration = 1\n",
    "            losses = []\n",
    "            for i in range(num_epoch):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(train, self.num_unrollings, self.batch_size):\n",
    "                    \n",
    "                    #Set input values for model placeholders\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y.reshape(self.batch_size,1),\n",
    "                            'tf_keep_prob:0': 0.8,\n",
    "                            'tf_learning_rate:0': lr_array[i],\n",
    "                            self.initial_state: state}\n",
    "\n",
    "                    #Run the training for 1 iteration\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['train_cost_mse:0', 'train_op', self.final_state],\n",
    "                            feed_dict=feed)\n",
    "                    losses.append(loss)\n",
    "                    #Print info about epoch loss\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               i + 1, num_epoch,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    #Add 1 to iteration counter\n",
    "                    iteration +=1\n",
    "                #Save the model after 10 epochs\n",
    "                if (i+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/price_prediction-%d.ckpt\" % i)\n",
    "\n",
    "            self.saver.save(sess, \"model/price_prediction-%d.ckpt\" % i)\n",
    "            return losses\n",
    "\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        preds = []\n",
    "        error = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for batch_x, batch_y in create_batch_generator(x_data, self.num_unrollings, self.batch_size):\n",
    "                feed = {'tf_x:0': batch_x,\n",
    "                        'tf_y:0': batch_y.reshape(self.batch_size,1),\n",
    "                        'tf_keep_prob:0': 0.8,\n",
    "                        self.initial_state: test_state}\n",
    "\n",
    "                pred, loss, test_state = sess.run(\n",
    "                    ['predictions:0','train_cost_mse:0', self.final_state],\n",
    "                    feed_dict=feed\n",
    "                )\n",
    "                preds.append(pred.tolist())\n",
    "                error.append(loss)\n",
    "\n",
    "        return preds, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_aapl = LSTMStockPredictor(aapl_train.shape[1], \n",
    "                              lstm_size=hy_lstm_size, \n",
    "                              batch_size=hy_batch_size, \n",
    "                              num_unrollings=hy_num_unrolling, \n",
    "                              learning_rate=hy_learning_rate, \n",
    "                              learning_rate_decay=hy_learning_rate_decay, \n",
    "                              init_epoch_decay=hy_init_epoch_decay)\n",
    "\n",
    "losses_aapl = rnn_aapl.train(aapl_train, num_epoch=10)\n",
    "\n",
    "predictions_aapl, losses_pred_aapl = rnn_aapl.predict(aapl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.title(\"Loss Mean Squared Error - Apple\")\n",
    "plt.plot(np.arange(len(losses_aapl)), losses_aapl)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fix = []\n",
    "for i in range(len(predictions_aapl)):\n",
    "    for z in range(len(predictions_aapl[i])):\n",
    "        predictions_fix.append(predictions_aapl[i][z][0])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Apple\")\n",
    "\n",
    "plt.plot(range(aapl_test.shape[0]), aapl_test[:,0], label = 'Actual')\n",
    "plt.plot(range(hy_batch_size,len(predictions_fix)+hy_batch_size), predictions_fix, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"US Dollar(Normalized)\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(range(0, len(predictions_fix), hy_batch_size), losses_pred_aapl, label='Prediction Losses')\n",
    "plt.xlabel(\"Time index\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "aapl_average_loss = sum(losses_pred_aapl)/(len(losses_pred_aapl))\n",
    "print(\"Average MSE testing: \" + str(aapl_average_loss))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_msft = LSTMStockPredictor(msft_train.shape[1], \n",
    "                              lstm_size=hy_lstm_size, \n",
    "                              batch_size=hy_batch_size, \n",
    "                              num_unrollings=hy_num_unrolling, \n",
    "                              learning_rate=hy_learning_rate, \n",
    "                              learning_rate_decay=hy_learning_rate_decay, \n",
    "                              init_epoch_decay=hy_init_epoch_decay)\n",
    "\n",
    "losses_msft = rnn_msft.train(msft_train, num_epoch=10)\n",
    "\n",
    "predictions_msft, losses_pred_msft = rnn_msft.predict(msft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.title(\"Loss Mean Squared Error - Microsoft\")\n",
    "plt.plot(np.arange(len(losses_msft)), losses_msft)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fix = []\n",
    "for i in range(len(predictions_msft)):\n",
    "    for z in range(len(predictions_msft[i])):\n",
    "        predictions_fix.append(predictions_msft[i][z][0])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Microsoft\")\n",
    "\n",
    "plt.plot(range(msft_test.shape[0]), msft_test[:,0], label = 'Actual')\n",
    "plt.plot(range(hy_batch_size,len(predictions_fix)+hy_batch_size), predictions_fix, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"US Dollar(Normalized)\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(range(0, len(predictions_fix), hy_batch_size), losses_pred_msft, label='Prediction Losses')\n",
    "plt.xlabel(\"Time index\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "msft_avg_loss = sum(losses_pred_msft)/(len(losses_pred_msft))\n",
    "print(\"Average MSE testing: \" + str(msft_avg_loss))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
